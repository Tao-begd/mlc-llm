





<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Python API and Gradio Frontend &mdash; mlc-llm 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/tabs.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="iOS App and Swift API" href="ios.html" />
    <link rel="prev" title="CLI and C++ API" href="cli.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://llm.mlc.ai/>Home</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/mlc-ai/mlc-llm>Github</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://discord.gg/9Xpy2HGBuD>Discord Server</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   Other Resources
                 </button>
                 <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  Other Resources
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/mlc-logo-with-text-landscape.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
                <div class="version">
                  0.1.0
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started/project_overview.html">Project Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/mlc_chat_config.html">Configure MLCChat in JSON</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build and Deploy Apps</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="javascript.html">WebLLM and Javascript API</a></li>
<li class="toctree-l1"><a class="reference internal" href="rest.html">Rest API</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">CLI and C++ API</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Python API and Gradio Frontend</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#python-api">Python API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#verify-installation">Verify Installation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#get-started">Get Started</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tutorial-with-python-notebooks">Tutorial with Python Notebooks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#configure-mlcchat-in-python">Configure MLCChat in Python</a></li>
<li class="toctree-l3"><a class="reference internal" href="#raw-text-generation-in-python">Raw Text Generation in Python</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stream-iterator-in-python">Stream Iterator in Python</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#api-reference">API Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="#gradio-frontend">Gradio Frontend</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ios.html">iOS App and Swift API</a></li>
<li class="toctree-l1"><a class="reference internal" href="android.html">Android App</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compile Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../compilation/compile_models.html">Compile Models via MLC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/distribute_compiled_models.html">Distribute Compiled Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/python.html">Python API for Model Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/configure_quantization.html">ðŸš§ Configure Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Define Model Architectures</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/customize/define_new_models.html">Define New Model Architectures</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Prebuilt Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../prebuilt_models.html">Model Prebuilts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Dependency Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/tvm.html">Install TVM Unity Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/mlc_llm.html">Install MLC LLM Python Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/conda.html">Install Conda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/gpu.html">GPU Drivers and SDKs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/emcc.html">Install Wasm Build Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../community/guideline.html">Community Guideline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/faq.html">Frequently Asked Questions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../privacy.html">MLC Chat App Privacy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- mlc-llm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>Python API and Gradio Frontend</li>
    
    
      
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/mlc-ai/mlc-llm/edit/main/docs/deploy/python.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="python-api-and-gradio-frontend">
<h1>Python API and Gradio Frontend<a class="headerlink" href="#python-api-and-gradio-frontend" title="Permalink to this heading">Â¶</a></h1>
<nav class="contents local" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#python-api" id="id1">Python API</a></p>
<ul>
<li><p><a class="reference internal" href="#verify-installation" id="id2">Verify Installation</a></p></li>
<li><p><a class="reference internal" href="#get-started" id="id3">Get Started</a></p></li>
<li><p><a class="reference internal" href="#tutorial-with-python-notebooks" id="id4">Tutorial with Python Notebooks</a></p></li>
<li><p><a class="reference internal" href="#configure-mlcchat-in-python" id="id5">Configure MLCChat in Python</a></p></li>
<li><p><a class="reference internal" href="#raw-text-generation-in-python" id="id6">Raw Text Generation in Python</a></p></li>
<li><p><a class="reference internal" href="#stream-iterator-in-python" id="id7">Stream Iterator in Python</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#api-reference" id="id8">API Reference</a></p></li>
<li><p><a class="reference internal" href="#gradio-frontend" id="id9">Gradio Frontend</a></p></li>
</ul>
</nav>
<p>We expose Python API for the MLC-Chat for easy integration into other Python projects.
We also provide a web demo based on <a class="reference external" href="https://gradio.app/">gradio</a> as an example of using Python API to interact with MLC-Chat.</p>
<section id="python-api">
<h2><a class="toc-backref" href="#id1" role="doc-backlink">Python API</a><a class="headerlink" href="#python-api" title="Permalink to this heading">Â¶</a></h2>
<p>The Python API is a part of the MLC-Chat package, which we have prepared pre-built pip wheels via the <a class="reference internal" href="../install/mlc_llm.html"><span class="doc">installation page</span></a>.</p>
<section id="verify-installation">
<h3><a class="toc-backref" href="#id2" role="doc-backlink">Verify Installation</a><a class="headerlink" href="#verify-installation" title="Permalink to this heading">Â¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;from mlc_chat import ChatModule; print(ChatModule)&quot;</span>
</pre></div>
</div>
<p>You are expected to see the information about the <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_chat.ChatModule</span></code> class.</p>
<p>If the prebuilt is unavailable on your platform, or you would like to build a runtime
that supports other GPU runtime than the prebuilt version. Please refer our <a class="reference internal" href="rest.html#mlcchat-package-build-from-source"><span class="std std-ref">Build MLC-Chat Package From Source</span></a> tutorial.</p>
</section>
<section id="get-started">
<h3><a class="toc-backref" href="#id3" role="doc-backlink">Get Started</a><a class="headerlink" href="#get-started" title="Permalink to this heading">Â¶</a></h3>
<p>After confirming that the package <code class="docutils literal notranslate"><span class="pre">mlc_chat</span></code> is installed, we can follow the steps
below to chat with an MLC-compiled model in Python.</p>
<p>First, let us make sure that the MLC-compiled <code class="docutils literal notranslate"><span class="pre">model</span></code> we want to chat with already exists.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">model</span></code> has the format <code class="docutils literal notranslate"><span class="pre">f&quot;{model_name}-{quantize_mode}&quot;</span></code>. For instance, if
you used <code class="docutils literal notranslate"><span class="pre">q4f16_1</span></code> as the <code class="docutils literal notranslate"><span class="pre">quantize_mode</span></code> to compile <code class="docutils literal notranslate"><span class="pre">Llama-2-7b-chat-hf</span></code>, you
would have <code class="docutils literal notranslate"><span class="pre">model</span></code> being <code class="docutils literal notranslate"><span class="pre">Llama-2-7b-chat-hf-q4f16_1</span></code>.</p>
</div>
<p>If you do not have the MLC-compiled <code class="docutils literal notranslate"><span class="pre">model</span></code> ready:</p>
<ul class="simple">
<li><p>Checkout <a class="reference internal" href="../index.html#get-started"><span class="std std-ref">Try out MLC Chat</span></a> to download prebuilt models for simplicity, or</p></li>
<li><p>Checkout <a class="reference internal" href="../compilation/compile_models.html#compile-models-via-mlc"><span class="std std-ref">Compile Models via MLC</span></a> to compile models with <code class="docutils literal notranslate"><span class="pre">mlc_llm</span></code> (another package) yourself</p></li>
</ul>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-0-0-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-0-0-0" name="0-0" role="tab" tabindex="0">Check prebuilt models</button><button aria-controls="panel-0-0-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-1" name="0-1" role="tab" tabindex="-1">Check compiled models</button></div><div aria-labelledby="tab-0-0-0" class="sphinx-tabs-panel" id="panel-0-0-0" name="0-0" role="tabpanel" tabindex="0"><p>If you downloaded prebuilt models from MLC LLM, by default:</p>
<ul class="simple">
<li><p>Model lib should be placed at <code class="docutils literal notranslate"><span class="pre">./dist/prebuilt/lib/$(model)-$(arch).$(suffix)</span></code>.</p></li>
<li><p>Model weights and chat config are located under <code class="docutils literal notranslate"><span class="pre">./dist/prebuilt/mlc-chat-$(model)/</span></code>.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please make sure that you have the same directory structure as above, because Python API
relies on it to automatically search for model lib and weights. If you would like to directly
provide a full model lib path to override the auto-search, you can specify <code class="docutils literal notranslate"><span class="pre">ChatModule.model_lib_path</span></code></p>
</div>
<details class="summary-example">
<summary>Example</summary><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt;<span class="w"> </span>ls<span class="w"> </span>-l<span class="w"> </span>./dist/prebuilt/lib
Llama-2-7b-chat-hf-q4f16_1-metal.so<span class="w">  </span><span class="c1"># Format: $(model)-$(arch).$(suffix)</span>
Llama-2-7b-chat-hf-q4f16_1-vulkan.so
...
&gt;&gt;&gt;<span class="w"> </span>ls<span class="w"> </span>-l<span class="w"> </span>./dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1<span class="w">  </span><span class="c1"># Format: ./dist/prebuilt/mlc-chat-$(model)/</span>
<span class="c1"># chat config:</span>
mlc-chat-config.json
<span class="c1"># model weights:</span>
ndarray-cache.json
params_shard_*.bin
...
</pre></div>
</div>
</details></div><div aria-labelledby="tab-0-0-1" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-1" name="0-1" role="tabpanel" tabindex="0"><p>If you have compiled models using MLC LLM, by default:</p>
<ul class="simple">
<li><p>Model libraries should be placed at <code class="docutils literal notranslate"><span class="pre">./dist/$(model)/$(model)-$(arch).$(suffix)</span></code>.</p></li>
<li><p>Model weights and chat config are located under <code class="docutils literal notranslate"><span class="pre">./dist/$(model)/params/</span></code>.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please make sure that you have the same directory structure as above, because Python API
relies on it to automatically search for model lib and weights. If you would like to directly
provide a full model lib path to override the auto-search, you can specify <code class="docutils literal notranslate"><span class="pre">ChatModule.model_lib_path</span></code></p>
</div>
<details class="summary-example">
<summary>Example</summary><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt;<span class="w"> </span>ls<span class="w"> </span>-l<span class="w"> </span>./dist/Llama-2-7b-chat-hf-q4f16_1/<span class="w"> </span><span class="c1"># Format: ./dist/$(model)/</span>
Llama-2-7b-chat-hf-q4f16_1-metal.so<span class="w">  </span><span class="c1"># Format: $(model)-$(arch).$(suffix)</span>
...
&gt;&gt;&gt;<span class="w"> </span>ls<span class="w"> </span>-l<span class="w"> </span>./dist/Llama-2-7b-chat-hf-q4f16_1/params<span class="w">  </span><span class="c1"># Format: ``./dist/$(model)/params/``</span>
<span class="c1"># chat config:</span>
mlc-chat-config.json
<span class="c1"># model weights:</span>
ndarray-cache.json
params_shard_*.bin
...
</pre></div>
</div>
</details></div></div>
<p>After making sure that the files exist, use the conda environment you used
to install <code class="docutils literal notranslate"><span class="pre">mlc_chat</span></code>, from the <code class="docutils literal notranslate"><span class="pre">mlc-llm</span></code> directory, you can create a Python
file <code class="docutils literal notranslate"><span class="pre">sample_mlc_chat.py</span></code> and paste the following lines:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_chat</span> <span class="kn">import</span> <span class="n">ChatModule</span>
<span class="kn">from</span> <span class="nn">mlc_chat.callback</span> <span class="kn">import</span> <span class="n">StreamToStdout</span>

<span class="c1"># From the mlc-llm directory, run</span>
<span class="c1"># $ python sample_mlc_chat.py</span>

<span class="c1"># Create a ChatModule instance</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">ChatModule</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;Llama-2-7b-chat-hf-q4f16_1&quot;</span><span class="p">)</span>
<span class="c1"># You can change to other models that you downloaded, for example,</span>
<span class="c1"># cm = ChatModule(model=&quot;Llama-2-13b-chat-hf-q4f16_1&quot;)  # Llama2 13b model</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
   <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;What is the meaning of life?&quot;</span><span class="p">,</span>
   <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Print prefill and decode performance statistics</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Statistics: </span><span class="si">{</span><span class="n">cm</span><span class="o">.</span><span class="n">stats</span><span class="p">()</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
   <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;How many points did you list out?&quot;</span><span class="p">,</span>
   <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Reset the chat module by</span>
<span class="c1"># cm.reset_chat()</span>
</pre></div>
</div>
<p>Now run the Python file to start the chat</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>sample_mlc_chat.py
</pre></div>
</div>
<p>You can also checkout the <a class="reference internal" href="../prebuilt_models.html"><span class="doc">Model Prebuilts</span></a> page to run other models.</p>
<details class="summary-see-output">
<summary>See output</summary><div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Using model folder: ./dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1
Using mlc chat config: ./dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1/mlc-chat-config.json
Using library model: ./dist/prebuilt/lib/Llama-2-7b-chat-hf-q4f16_1-cuda.so

Thank you for your question! The meaning of life is a complex and subjective topic that has been debated by philosophers, theologians, scientists, and many others for centuries. There is no one definitive answer to this question, as it can vary depending on a person&#39;s beliefs, values, experiences, and perspectives.

However, here are some possible ways to approach the question:

1. Religious or spiritual beliefs: Many people believe that the meaning of life is to fulfill a divine or spiritual purpose, whether that be to follow a set of moral guidelines, to achieve spiritual enlightenment, or to fulfill a particular destiny.
2. Personal growth and development: Some people believe that the meaning of life is to learn, grow, and evolve as individuals, to develop one&#39;s talents and abilities, and to become the best version of oneself.
3. Relationships and connections: Others believe that the meaning of life is to form meaningful connections and relationships with others, to love and be loved, and to build a supportive and fulfilling social network.
4. Contribution and impact: Some people believe that the meaning of life is to make a positive impact on the world, to contribute to society in a meaningful way, and to leave a lasting legacy.
5. Simple pleasures and enjoyment: Finally, some people believe that the meaning of life is to simply enjoy the present moment, to find pleasure and happiness in the simple things in life, and to appreciate the beauty and wonder of the world around us.

Ultimately, the meaning of life is a deeply personal and subjective question, and each person must find their own answer based on their own beliefs, values, and experiences.

Statistics: prefill: 3477.5 tok/s, decode: 153.6 tok/s

I listed out 5 possible ways to approach the question of the meaning of life.
</pre></div>
</div>
</details><div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You could also specify the address of <code class="docutils literal notranslate"><span class="pre">model</span></code> and <code class="docutils literal notranslate"><span class="pre">model_lib_path</span></code> explicitly. If
you only specify <code class="docutils literal notranslate"><span class="pre">model</span></code> as <code class="docutils literal notranslate"><span class="pre">model_name</span></code> and <code class="docutils literal notranslate"><span class="pre">quantize_mode</span></code>, we will
do a search for you. See more in the documentation of <code class="xref py py-meth docutils literal notranslate"><span class="pre">mlc_chat.ChatModule.__init__()</span></code>.</p>
</div>
</section>
<section id="tutorial-with-python-notebooks">
<h3><a class="toc-backref" href="#id4" role="doc-backlink">Tutorial with Python Notebooks</a><a class="headerlink" href="#tutorial-with-python-notebooks" title="Permalink to this heading">Â¶</a></h3>
<p>Now that you have tried out how to chat with the model in Python, we would
recommend you to checkout the following tutorials in Python notebook (all runnable in Colab):</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_chat_module_getting_started.ipynb">Getting Started with MLC-LLM</a>:
how to quickly download prebuilt models and chat with it</p></li>
<li><p><a class="reference external" href="https://github.com/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_compile_llama2_with_mlc_llm.ipynb">Compiling Llama-2 with MLC-LLM</a>:
how to use Python APIs to compile models with the MLC-LLM workflow</p></li>
<li><p><a class="reference external" href="https://github.com/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_extensions_to_more_model_variants.ipynb">Extensions to More Model Variants</a>:
how to use Python APIs to compile and chat with any model variant youâ€™d like</p></li>
<li><p><a class="reference external" href="https://github.com/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_raw_text_generation.ipynb">Raw Text Generation with MLC-LLM</a>:
how to perform raw text generation with MLC-LLM in Python</p></li>
</ul>
</section>
<section id="configure-mlcchat-in-python">
<h3><a class="toc-backref" href="#id5" role="doc-backlink">Configure MLCChat in Python</a><a class="headerlink" href="#configure-mlcchat-in-python" title="Permalink to this heading">Â¶</a></h3>
<p>If you have checked out <a class="reference internal" href="../get_started/mlc_chat_config.html#configure-mlc-chat-json"><span class="std std-ref">Configure MLCChat in JSON</span></a>, you would know
that you could configure MLCChat through various fields such as <code class="docutils literal notranslate"><span class="pre">temperature</span></code>. We provide the
option of overriding any field youâ€™d like in Python, so that you do not need to manually edit
<code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>.</p>
<p>Since there are two concepts â€“ <cite>MLCChat Configuration</cite> and <cite>Conversation Configuration</cite> â€“ we correspondingly
provide two dataclasses <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_chat.ChatConfig</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_chat.ConvConfig</span></code>.</p>
<p>We provide an example below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_chat</span> <span class="kn">import</span> <span class="n">ChatModule</span><span class="p">,</span> <span class="n">ChatConfig</span><span class="p">,</span> <span class="n">ConvConfig</span>
<span class="kn">from</span> <span class="nn">mlc_chat.callback</span> <span class="kn">import</span> <span class="n">StreamToStdout</span>

<span class="c1"># Using a `ConvConfig`, we modify `system`, a field in the conversation template</span>
<span class="c1"># `system` refers to the prompt encoded before starting the chat</span>
<span class="n">conv_config</span> <span class="o">=</span> <span class="n">ConvConfig</span><span class="p">(</span><span class="n">system</span><span class="o">=</span><span class="s1">&#39;Please show as much happiness as you can when talking to me.&#39;</span><span class="p">)</span>

<span class="c1"># We then include the `ConvConfig` instance in `ChatConfig` while overriding `max_gen_len`</span>
<span class="c1"># Note that `conv_config` is an optional subfield of `chat_config`</span>
<span class="n">chat_config</span> <span class="o">=</span> <span class="n">ChatConfig</span><span class="p">(</span><span class="n">max_gen_len</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">conv_config</span><span class="o">=</span><span class="n">conv_config</span><span class="p">)</span>

<span class="c1"># Using the `chat_config` we created, instantiate a `ChatModule`</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">mlc_chat</span><span class="o">.</span><span class="n">ChatModule</span><span class="p">(</span><span class="s1">&#39;Llama-2-7b-chat-hf-q4f16_1&#39;</span><span class="p">,</span> <span class="n">chat_config</span><span class="o">=</span><span class="n">chat_config</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
   <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;What is one plus one?&quot;</span><span class="p">,</span>
   <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># You could also pass in a `ConvConfig` instance to `reset_chat()`</span>
<span class="n">conv_config</span> <span class="o">=</span> <span class="n">ConvConfig</span><span class="p">(</span><span class="n">system</span><span class="o">=</span><span class="s1">&#39;Please show as much sadness as you can when talking to me.&#39;</span><span class="p">)</span>
<span class="n">chat_config</span> <span class="o">=</span> <span class="n">ChatConfig</span><span class="p">(</span><span class="n">max_gen_len</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">conv_config</span><span class="o">=</span><span class="n">conv_config</span><span class="p">)</span>
<span class="n">cm</span><span class="o">.</span><span class="n">reset_chat</span><span class="p">(</span><span class="n">chat_config</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
   <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;What is one plus one?&quot;</span><span class="p">,</span>
   <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<details class="summary-see-output">
<summary>See output</summary><div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Using model folder: ./dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1
Using mlc chat config: ./dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1/mlc-chat-config.json
Using library model: ./dist/prebuilt/lib/Llama-2-7b-chat-hf-q4f16_1-cuda.so

Oh, wow, *excitedly* one plus one? *grinning* Well, let me see... *counting on fingers* One plus one is... *eureka* Two!
...

*Sobs* Oh, the tragedy of it all... *sobs* One plus one... *chokes back tears* It&#39;s... *gulps* it&#39;s... *breaks down in tears* TWO!
...
</pre></div>
</div>
</details><div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You do not need to specify the entire <code class="docutils literal notranslate"><span class="pre">ChatConfig</span></code> or <code class="docutils literal notranslate"><span class="pre">ConvConfig</span></code>. Instead, we will first
load all the fields defined in <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>, a file required when instantiating
a <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_chat.ChatModule</span></code>. Then, we will load in the optional <code class="docutils literal notranslate"><span class="pre">ChatConfig</span></code> you provide, overriding the
fields specified.</p>
<p>It is also worth noting that <code class="docutils literal notranslate"><span class="pre">ConvConfig</span></code> itself is overriding the original conversation template
specified by the field <code class="docutils literal notranslate"><span class="pre">conv_template</span></code> in the chat configuration. Learn more about it in
<a class="reference internal" href="../get_started/mlc_chat_config.html#configure-mlc-chat-json"><span class="std std-ref">Configure MLCChat in JSON</span></a>.</p>
</div>
</section>
<section id="raw-text-generation-in-python">
<h3><a class="toc-backref" href="#id6" role="doc-backlink">Raw Text Generation in Python</a><a class="headerlink" href="#raw-text-generation-in-python" title="Permalink to this heading">Â¶</a></h3>
<p>Raw text generation allows the user to have more flexibility over his prompts,
without being forced to create a new conversational template, making prompt customization easier.
This serves other demands for APIs to handle LLM generation without the usual system prompts and other items.</p>
<p>We provide an example below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_chat</span> <span class="kn">import</span> <span class="n">ChatModule</span><span class="p">,</span> <span class="n">ChatConfig</span><span class="p">,</span> <span class="n">ConvConfig</span>
<span class="kn">from</span> <span class="nn">mlc_chat.callback</span> <span class="kn">import</span> <span class="n">StreamToStdout</span>

<span class="c1"># Use a `ConvConfig` to define the generation settings</span>
<span class="c1"># Since the &quot;LM&quot; template only supports raw text generation,</span>
<span class="c1"># System prompts will not be executed even if provided</span>
<span class="n">conv_config</span> <span class="o">=</span> <span class="n">ConvConfig</span><span class="p">(</span><span class="n">stop_tokens</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,],</span> <span class="n">add_bos</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">stop_str</span><span class="o">=</span><span class="s2">&quot;[INST]&quot;</span><span class="p">)</span>

<span class="c1"># Note that `conv_config` is an optional subfield of `chat_config`</span>
<span class="c1"># The &quot;LM&quot; template serves the basic purposes of raw text generation</span>
<span class="n">chat_config</span> <span class="o">=</span> <span class="n">ChatConfig</span><span class="p">(</span><span class="n">conv_config</span><span class="o">=</span><span class="n">conv_config</span><span class="p">,</span> <span class="n">conv_template</span><span class="o">=</span><span class="s2">&quot;LM&quot;</span><span class="p">)</span>

<span class="c1"># Using the `chat_config` we created, instantiate a `ChatModule`</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">ChatModule</span><span class="p">(</span><span class="s1">&#39;Llama-2-7b-chat-hf-q4f16_1&#39;</span><span class="p">,</span> <span class="n">chat_config</span><span class="o">=</span><span class="n">chat_config</span><span class="p">)</span>

<span class="c1"># To make the model follow conversations a chat structure should be provided</span>
<span class="c1"># This allows users to build their own prompts without building a new template</span>
<span class="n">system_prompt</span> <span class="o">=</span> <span class="s2">&quot;&lt;&lt;SYS&gt;&gt;</span><span class="se">\n</span><span class="s2">You are a helpful, respectful and honest assistant.</span><span class="se">\n</span><span class="s2">&lt;&lt;/SYS&gt;&gt;</span><span class="se">\n\n</span><span class="s2">&quot;</span>
<span class="n">inst_prompt</span> <span class="o">=</span> <span class="s2">&quot;What is mother nature?&quot;</span>

<span class="c1"># Concatenate system and instruction prompts, and add instruction tags</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
   <span class="n">prompt</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;[INST] </span><span class="si">{</span><span class="n">system_prompt</span><span class="o">+</span><span class="n">inst_prompt</span><span class="si">}</span><span class="s2"> [/INST]&quot;</span><span class="p">,</span>
   <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># The LM template has no memory, so it will be reset every single generation</span>
<span class="c1"># In this case the model will just follow normal text completion</span>
<span class="c1"># because there isn&#39;t a chat structure</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
   <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;Life is a quality that distinguishes&quot;</span><span class="p">,</span>
   <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">LM</span></code> is a template without memory, which means that every execution will be cleared.
Additionally, system prompts will not be run when instantiating a <cite>mlc_chat.ChatModule</cite>,
unless explicitly given inside the prompt.</p>
</div>
</section>
<section id="stream-iterator-in-python">
<h3><a class="toc-backref" href="#id7" role="doc-backlink">Stream Iterator in Python</a><a class="headerlink" href="#stream-iterator-in-python" title="Permalink to this heading">Â¶</a></h3>
<p>Stream Iterator gives users an option to stream generated text to the function that the API is called from,
instead of streaming to stdout, which could be a necessity when building services on top of MLC Chat.</p>
<p>We provide an example below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_chat</span> <span class="kn">import</span> <span class="n">ChatModule</span>
<span class="kn">from</span> <span class="nn">mlc_chat.callback</span> <span class="kn">import</span> <span class="n">StreamIterator</span>

<span class="c1"># Create a ChatModule instance</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">ChatModule</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;Llama-2-7b-chat-hf-q4f16_1&quot;</span><span class="p">)</span>

<span class="c1"># Stream to an Iterator</span>
<span class="kn">from</span> <span class="nn">threading</span> <span class="kn">import</span> <span class="n">Thread</span>

<span class="n">stream</span> <span class="o">=</span> <span class="n">StreamIterator</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">generation_thread</span> <span class="o">=</span> <span class="n">Thread</span><span class="p">(</span>
   <span class="n">target</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">,</span>
   <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the meaning of life?&quot;</span><span class="p">,</span> <span class="s2">&quot;progress_callback&quot;</span><span class="p">:</span> <span class="n">stream</span><span class="p">},</span>
<span class="p">)</span>
<span class="n">generation_thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

<span class="n">output</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
<span class="k">for</span> <span class="n">delta_message</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
   <span class="n">output</span> <span class="o">+=</span> <span class="n">delta_message</span>

<span class="n">generation_thread</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="api-reference">
<h2><a class="toc-backref" href="#id8" role="doc-backlink">API Reference</a><a class="headerlink" href="#api-reference" title="Permalink to this heading">Â¶</a></h2>
<p>User can initiate a chat module by creating <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_chat.ChatModule</span></code> class, which is a wrapper of the MLC-Chat model.
The <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_chat.ChatModule</span></code> class provides the following methods:</p>
</section>
<section id="gradio-frontend">
<h2><a class="toc-backref" href="#id9" role="doc-backlink">Gradio Frontend</a><a class="headerlink" href="#gradio-frontend" title="Permalink to this heading">Â¶</a></h2>
<p>The gradio frontend provides a web interface for the MLC-Chat model, which allows users to interact with the model in a more user-friendly way and switch between different models to compare performance.
To use gradio frontend, you need to install gradio first:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>gradio
</pre></div>
</div>
<p>Then you can run the following code to start the interface:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>mlc_chat.gradio<span class="w"> </span>--artifact-path<span class="w"> </span>ARTIFACT_PATH<span class="w"> </span><span class="o">[</span>--device<span class="w"> </span>DEVICE<span class="o">]</span><span class="w"> </span><span class="o">[</span>--port<span class="w"> </span>PORT_NUMBER<span class="o">]</span><span class="w"> </span><span class="o">[</span>--share<span class="o">]</span>
</pre></div>
</div>
<dl class="option-list">
<dt><kbd><span class="option">--artifact-path</span></kbd></dt>
<dd><p>Please provide a path containing all the model folders you wish to use. The default value is <code class="docutils literal notranslate"><span class="pre">dist</span></code>.</p>
</dd>
<dt><kbd><span class="option">--device</span></kbd></dt>
<dd><p>The description of the device to run on. User should provide a string in the form of â€˜device_name:device_idâ€™ or â€˜device_nameâ€™, where â€˜device_nameâ€™ is one of â€˜cudaâ€™, â€˜metalâ€™, â€˜vulkanâ€™, â€˜rocmâ€™, â€˜openclâ€™, â€˜autoâ€™ (automatically detect the local device), and â€˜device_idâ€™ is the device id to run on. If no â€˜device_idâ€™ is provided, it will be set to 0. The default value is <code class="docutils literal notranslate"><span class="pre">auto</span></code>.</p>
</dd>
<dt><kbd><span class="option">--port</span></kbd></dt>
<dd><p>The port number to run gradio. The default value is <code class="docutils literal notranslate"><span class="pre">7860</span></code>.</p>
</dd>
<dt><kbd><span class="option">--share</span></kbd></dt>
<dd><p>Whether to create a publicly shareable link for the interface.</p>
</dd>
</dl>
<p>After setting it up properly, you are expected to see the following interface in your browser:</p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/gradio-interface.png"><img alt="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/gradio-interface.png" class="align-center" src="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/gradio-interface.png" style="width: 100%;" /></a>
</section>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="ios.html" class="btn btn-neutral float-right" title="iOS App and Swift API" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="cli.html" class="btn btn-neutral float-left" title="CLI and C++ API" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">Â© 2023 MLC LLM</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote"> </div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>